import json
import os
import time
from pathlib import Path
from typing import Optional
import re
import argparse
import sys
import subprocess
import logging

def ensure_dependencies(config: dict | None = None):
    """ƒê·∫£m b·∫£o c√°c package b·∫Øt bu·ªôc ƒë√£ c√≥. N·∫øu thi·∫øu s·∫Ω t·ª± ƒë·ªông c√†i b·∫±ng pip.

    C√†i g√≥i theo t·ª´ng module ƒë·ªÉ tr√°nh th·∫•t b·∫°i to√†n b·ªô khi m·ªôt package (v√≠ d·ª• faiss-cpu) kh√¥ng kh·∫£ d·ª•ng.
    """
    required = {
        "fastapi": "fastapi",
        "pydantic": "pydantic",
        "dotenv": "python-dotenv",
        "uvicorn": "uvicorn",
        "sentence_transformers": "sentence-transformers",
        "sklearn": "scikit-learn",
        "scipy": "scipy",
        "numpy": "numpy",
        "torch": "torch",
    }

    use_faiss = False
    try:
        use_faiss = bool(config.get("use_faiss", False)) if config else False
    except Exception:
        use_faiss = False

    def _try_import(modname: str) -> bool:
        try:
            __import__(modname)
            return True
        except Exception:
            return False

    def _install(pkg: str):
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", pkg])
        except subprocess.CalledProcessError:
            pass

    for mod, pkg in required.items():
        if not _try_import(mod):
            _install(pkg)

    if use_faiss and (not _try_import("faiss")):
        _install("faiss-cpu")

    critical = ["fastapi", "pydantic", "uvicorn", "sentence_transformers", "sklearn", "numpy"]
    missing = [m for m in critical if not _try_import(m)]
    if missing:
        raise RuntimeError(
            "Thi·∫øu c√°c th∆∞ vi·ªán b·∫Øt bu·ªôc: " + ", ".join(missing) +
            "\nVui l√≤ng ch·∫°y: pip install -r requirements.txt ho·∫∑c ƒë·ªÉ ch∆∞∆°ng tr√¨nh t·ª± c√†i ƒë·∫∑t c√≥ k·∫øt n·ªëi Internet."
        )

PROJECT_ROOT = Path(__file__).resolve().parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

def load_config() -> dict:
    """ƒê·ªçc config/settings.json lu√¥n theo ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi c·ªßa project root."""
    src_dir = Path(__file__).resolve().parent
    cfg_path = src_dir / "config" / "settings.json"
    with open(cfg_path, "r", encoding="utf-8") as f:
        return json.load(f)

config = load_config()

ensure_dependencies(config)

import uvicorn
from fastapi import FastAPI, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, HTMLResponse, Response
from pydantic import BaseModel
from dotenv import load_dotenv
from typing import List, Union

from src.rag_service import RagService
from src.clients.openai_client import OpenAIClient
from src.clients.ollama_client import OllamaClient
from src.utils.chunking import chunk_text
from src.utils.preprocess import preprocess_text

env_path = PROJECT_ROOT / ".env"
load_dotenv(dotenv_path=env_path)

# ÈÖçÁΩÆÊó•Âøó
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

app = FastAPI(title="MLN131 RAG Chatbot", version="2.0.0")

# C·∫•u h√¨nh CORS ƒë·ªÉ cho ph√©p frontend g·ªçi API
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Cho ph√©p t·∫•t c·∫£ origins (c√≥ th·ªÉ thay b·∫±ng danh s√°ch c·ª• th·ªÉ trong production)
    allow_credentials=False,  # Ph·∫£i False khi d√πng allow_origins=["*"]
    allow_methods=["*"],  # Cho ph√©p t·∫•t c·∫£ methods
    allow_headers=["*"],  # Cho ph√©p t·∫•t c·∫£ headers
)

rag: RagService = RagService(config)
llm_client: Union[OpenAIClient, OllamaClient, None] = None


class QueryRequest(BaseModel):
    question: str
    top_k: Optional[int] = None
    image_urls: Optional[List[str]] = None
    file_urls: Optional[List[str]] = None
    use_websearch: Optional[bool] = False


class RebuildRequest(BaseModel):
    backend: Optional[str] = None


@app.on_event("startup")
def startup_event():
    global llm_client
    model_type = os.getenv("MODEL_TYPE", "openai").lower()
    response_language = config.get("response_language", "vi")
    max_output_tokens = int(config.get("max_output_tokens", 400))
    temperature = float(config.get("temperature", 0.2))
    
    if model_type == "ollama":
        base_url = os.getenv("OLLAMA_BASE_URL", "http://server.nhotin.space:11434")
        model_name = os.getenv("OLLAMA_MODEL_NAME", "gpt-oss:20b")
        llm_client = OllamaClient(
            base_url=base_url,
            model_name=model_name,
            response_language=response_language,
            max_output_tokens=max_output_tokens,
            temperature=temperature,
        )
    else:
        api_key = os.getenv("OPENAI_API_KEY", "")
        model_name = os.getenv("OPENAI_MODEL_NAME", "gpt-4o-mini")
        if not api_key:
            raise ValueError("OPENAI_API_KEY kh√¥ng ƒë∆∞·ª£c t√¨m th·∫•y trong environment variables")
        llm_client = OpenAIClient(
            api_key=api_key,
            model_name=model_name,
            response_language=response_language,
            max_output_tokens=max_output_tokens,
            temperature=temperature,
        )
    
    try:
        rag.load_index()
    except Exception:
        project_root = Path(__file__).resolve().parent.parent
        data_path_cfg = config.get("data_path", "data/data.txt")
        data_path = (project_root / data_path_cfg) if not Path(data_path_cfg).is_absolute() else Path(data_path_cfg)
        if not data_path.exists():
            raise FileNotFoundError(f"Kh√¥ng th·∫•y file d·ªØ li·ªáu: {data_path}")
        text = data_path.read_text(encoding="utf-8")
        text = preprocess_text(text)
        chunks = chunk_text(
            text,
            chunk_size=int(config.get("chunk_size", 800)),
            chunk_overlap=int(config.get("chunk_overlap", 120)),
            separators=config.get("separators", None),
            source=str(data_path)
        )
        rag.build_index(chunks)
        rag.load_index()


@app.get("/health")
def health():
    chunk_count = 0
    try:
        chunk_count = len(rag.docstore)
    except Exception:
        pass
    return {"status": "ok", "index_ready": rag.is_ready(), "chunk_count": chunk_count}


def _is_about_maclenin(question: str) -> bool:
    """Ki·ªÉm tra xem c√¢u h·ªèi c√≥ li√™n quan ƒë·∫øn maclenin (c·∫•u h√¨nh chatbot) kh√¥ng."""
    question_lower = question.lower()
    keywords = [
        "maclenin", "m√°cl√™nin", 
        "c·∫•u h√¨nh", "c·∫•u h√¨nh chatbot", 
        "chatbot c·ªßa b·∫°n", 
        "b·∫°n l√† ai", "who are you", "what is your name",
        "gi·ªõi thi·ªáu v·ªÅ b·∫°n", "tell me about",
        "bot n√†y", "bot t√™n", "t√™n g√¨", "t√™n b·∫°n",
        "b·∫°n l√†m g√¨", "what do you do"
    ]
    return any(keyword in question_lower for keyword in keywords)


def _get_bot_config_info() -> str:
    """Tr·∫£ v·ªÅ th√¥ng tin c·∫•u h√¨nh chatbot."""
    model_type = os.getenv("MODEL_TYPE", "openai").lower()
    
    info_parts = [
        "üëã Xin ch√†o! T√¥i l√† Maclenin, m·ªôt chatbot h·ªó tr·ª£ th√¥ng tin d·ª±a tr√™n RAG (Retrieval-Augmented Generation).",
        "",
        "üìã **C·∫•u h√¨nh hi·ªán t·∫°i:**",
        f"- **Lo·∫°i model:** {model_type.upper()}",
    ]
    
    if model_type == "ollama":
        base_url = os.getenv("OLLAMA_BASE_URL", "http://server.nhotin.space:11434")
        model_name = os.getenv("OLLAMA_MODEL_NAME", "gpt-oss:20b")
        info_parts.extend([
            f"- **Server Ollama:** {base_url}",
            f"- **Model:** {model_name}",
        ])
    else:
        model_name = os.getenv("OPENAI_MODEL_NAME", "gpt-4o-mini")
        info_parts.extend([
            f"- **Model:** {model_name}",
        ])
    
    info_parts.extend([
        "",
        "üîß **T√≠nh nƒÉng:**",
        "- T√¨m ki·∫øm th√¥ng tin t·ª´ database vector",
        "- Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n ng·ªØ c·∫£nh RAG",
    ])
    
    if model_type == "openai" and model_name.startswith("gpt-4.1"):
        info_parts.extend([
            "- H·ªó tr·ª£ web search",
            "- H·ªó tr·ª£ x·ª≠ l√Ω h√¨nh ·∫£nh v√† file",
        ])
    
    info_parts.append("")
    info_parts.append("üí° B·∫°n c√≥ th·ªÉ h·ªèi t√¥i b·∫•t k·ª≥ c√¢u h·ªèi n√†o li√™n quan ƒë·∫øn d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u tr·ªØ!")
    
    return "\n".join(info_parts)


@app.post("/query")
def query(req: QueryRequest):
    start = time.perf_counter()
    
    logger.info("=" * 80)
    logger.info(f"üìù NH·∫¨N C√ÇU H·ªéI: {req.question}")
    logger.info(f"üîß Tham s·ªë: top_k={req.top_k}, use_websearch={req.use_websearch}")
    
    # Ki·ªÉm tra n·∫øu c√¢u h·ªèi v·ªÅ maclenin th√¨ tr·∫£ v·ªÅ th√¥ng tin c·∫•u h√¨nh
    if _is_about_maclenin(req.question):
        logger.info("‚ÑπÔ∏è  C√¢u h·ªèi v·ªÅ bot config, tr·∫£ v·ªÅ th√¥ng tin c·∫•u h√¨nh")
        answer = _get_bot_config_info()
        elapsed_ms = int((time.perf_counter() - start) * 1000)
        logger.info(f"‚è±Ô∏è  Th·ªùi gian x·ª≠ l√Ω: {elapsed_ms}ms")
        logger.info("=" * 80)
        return {
            "question": req.question,
            "answer": answer,
            "contexts": [],
            "meta": {"type": "bot_info"},
            "latency_ms": elapsed_ms
        }
    
    contexts_for_llm = []
    
    # RAG Search
    logger.info("üîç B·∫ÆT ƒê·∫¶U RAG SEARCH...")
    results = rag.search(req.question, top_k=req.top_k)
    logger.info(f"üìä T√¨m th·∫•y {len(results)} k·∫øt qu·∫£ t·ª´ RAG search")
    
    # Log t·ª´ng k·∫øt qu·∫£
    for i, r in enumerate(results, 1):
        score = r.get("score", 0.0)
        source = r.get("source", "unknown")
        text_preview = r.get("text", "")[:100] + "..." if len(r.get("text", "")) > 100 else r.get("text", "")
        logger.info(f"  [{i}] Score: {score:.4f} | Source: {source}")
        logger.info(f"      Preview: {text_preview}")
    
    # Filter theo similarity threshold
    similarity_threshold = float(config.get("similarity_threshold", 0.6))
    logger.info(f"üéØ L·ªçc theo similarity threshold: {similarity_threshold}")
    filtered = [r for r in results if float(r.get("score", 0.0)) >= similarity_threshold]
    logger.info(f"‚úÖ Sau khi l·ªçc: {len(filtered)} k·∫øt qu·∫£ ƒë·∫°t ng∆∞·ª°ng")
    
    # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng contexts
    contexts_max = int(config.get("contexts_max", 3))
    contexts_for_llm = filtered[:contexts_max]
    logger.info(f"üì¶ Ch·ªçn {len(contexts_for_llm)} contexts ƒë·ªÉ g·ª≠i ƒë·∫øn LLM (max: {contexts_max})")
    
    # Log contexts ƒë∆∞·ª£c ch·ªçn
    for i, ctx in enumerate(contexts_for_llm, 1):
        logger.info(f"  Context {i}: Score={ctx.get('score', 0.0):.4f}, Source={ctx.get('source', 'unknown')}")
        logger.info(f"    Text: {ctx.get('text', '')[:200]}...")
    
    # Build prompt v√† g·ªçi LLM
    model_type = os.getenv("MODEL_TYPE", "openai").lower()
    logger.info(f"ü§ñ G·ªçi LLM: {model_type.upper()}")
    
    if model_type == "ollama":
        prompt = llm_client.build_prompt(req.question, contexts_for_llm)
        logger.info("üìÑ PROMPT ƒê∆Ø·ª¢C X√ÇY D·ª∞NG:")
        logger.info("-" * 80)
        logger.info(prompt[:1000] + "..." if len(prompt) > 1000 else prompt)
        logger.info("-" * 80)
        
        answer, meta = llm_client.answer(
            req.question, 
            contexts_for_llm
        )
    else:
        image_urls = req.image_urls or []
        file_urls = req.file_urls or []
        use_websearch = req.use_websearch or False
        
        prompt = llm_client.build_prompt(req.question, contexts_for_llm)
        logger.info("üìÑ PROMPT ƒê∆Ø·ª¢C X√ÇY D·ª∞NG:")
        logger.info("-" * 80)
        logger.info(prompt[:1000] + "..." if len(prompt) > 1000 else prompt)
        logger.info("-" * 80)
        
        if image_urls:
            logger.info(f"üñºÔ∏è  C√≥ {len(image_urls)} ·∫£nh ƒë∆∞·ª£c g·ª≠i k√®m")
        if file_urls:
            logger.info(f"üìé C√≥ {len(file_urls)} file ƒë∆∞·ª£c g·ª≠i k√®m")
        if use_websearch:
            logger.info("üåê Web search ƒë∆∞·ª£c b·∫≠t")
        
        answer, meta = llm_client.answer(
            req.question, 
            contexts_for_llm, 
            image_urls=image_urls if image_urls else None,
            file_urls=file_urls if file_urls else None,
            use_websearch=use_websearch
        )
    
    elapsed_ms = int((time.perf_counter() - start) * 1000)
    logger.info(f"üí¨ C√¢u tr·∫£ l·ªùi nh·∫≠n ƒë∆∞·ª£c (ƒë·ªô d√†i: {len(answer)} k√Ω t·ª±)")
    logger.info(f"‚è±Ô∏è  T·ªïng th·ªùi gian x·ª≠ l√Ω: {elapsed_ms}ms")
    logger.info("=" * 80)
    
    return {
        "question": req.question,
        "answer": answer,
        "contexts": contexts_for_llm,
        "meta": meta,
        "latency_ms": elapsed_ms
    }


def _wc(s: str) -> int:
    return len(re.findall(r"\w+", s))


@app.get("/chunks")
def chunks(limit: int = 3, preview_chars: int = 300):
    """Xem nhanh c√°c chunk ƒë√£ build (preview)."""
    limit = max(1, min(limit, 50))
    pcs = []
    for i, c in enumerate(rag.docstore[:limit]):
        txt = c.get("text", "")
        pcs.append({
            "id": i,
            "source": c.get("source", "unknown"),
            "word_count": _wc(txt),
            "preview": txt[:preview_chars]
        })
    return {"chunk_count": len(rag.docstore), "preview_count": len(pcs), "chunks": pcs}


@app.post("/admin/rebuild_index")
def rebuild_index(req: RebuildRequest):
    global rag
    cfg = load_config()
    if req.backend:
        cfg["backend"] = req.backend
    project_root = Path(__file__).resolve().parent.parent
    data_path_cfg = cfg.get("data_path", "data/data.txt")
    data_path = (project_root / data_path_cfg) if not Path(data_path_cfg).is_absolute() else Path(data_path_cfg)
    text = data_path.read_text(encoding="utf-8")
    text = preprocess_text(text)
    chunks = chunk_text(
        text,
        chunk_size=int(cfg.get("chunk_size", 800)),
        chunk_overlap=int(cfg.get("chunk_overlap", 120)),
        separators=cfg.get("separators", None),
        source=str(data_path)
    )
    new_rag = RagService(cfg)
    new_rag.build_index(chunks)
    new_rag.load_index()
    rag = new_rag
    return {"status": "rebuilt", "backend": cfg.get("backend"), "index_ready": rag.is_ready(), "chunks": len(chunks)}


@app.post("/query/upload")
async def query_with_upload(
    question: str = Form(...),
    file: Optional[UploadFile] = File(None),
    top_k: Optional[int] = Form(None),
    use_websearch: Optional[bool] = Form(False)
):
    """Query v·ªõi h·ªó tr·ª£ upload file (text/·∫£nh)."""
    start = time.perf_counter()
    
    logger.info("=" * 80)
    logger.info(f"üìù NH·∫¨N C√ÇU H·ªéI V·ªöI FILE UPLOAD: {question}")
    logger.info(f"üîß Tham s·ªë: top_k={top_k}, use_websearch={use_websearch}")
    
    # Ki·ªÉm tra n·∫øu c√¢u h·ªèi v·ªÅ maclenin th√¨ tr·∫£ v·ªÅ th√¥ng tin c·∫•u h√¨nh
    if _is_about_maclenin(question):
        logger.info("‚ÑπÔ∏è  C√¢u h·ªèi v·ªÅ bot config, tr·∫£ v·ªÅ th√¥ng tin c·∫•u h√¨nh")
        answer = _get_bot_config_info()
        elapsed_ms = int((time.perf_counter() - start) * 1000)
        logger.info(f"‚è±Ô∏è  Th·ªùi gian x·ª≠ l√Ω: {elapsed_ms}ms")
        logger.info("=" * 80)
        return {
            "question": question,
            "answer": answer,
            "contexts": [],
            "meta": {"type": "bot_info"},
            "latency_ms": elapsed_ms
        }
    
    contexts_for_llm = []
    image_urls = []
    
    if file:
        logger.info(f"üìé X·ª¨ L√ù FILE: {file.filename} (size: {file.size if hasattr(file, 'size') else 'unknown'} bytes)")
        content = await file.read()
        file_ext = file.filename.split('.')[-1].lower() if file.filename else ''
        logger.info(f"   Lo·∫°i file: {file_ext}")
        
        if file_ext in ['jpg', 'jpeg', 'png', 'gif', 'webp']:
            logger.info("   ‚Üí ƒê√¢y l√† file ·∫£nh, encode th√†nh base64")
            # Encode ·∫£nh th√†nh base64 ƒë·ªÉ g·ª≠i ƒë·∫øn OpenAI API
            import base64
            base64_image = base64.b64encode(content).decode('utf-8')
            mime_type = f"image/{file_ext}" if file_ext != 'jpg' else "image/jpeg"
            image_data_url = f"data:{mime_type};base64,{base64_image}"
            image_urls.append(image_data_url)
            logger.info(f"   ‚úÖ ƒê√£ encode ·∫£nh th√†nh base64 (length: {len(base64_image)} chars)")
        else:
            logger.info("   ‚Üí ƒê√¢y l√† file text, ƒë·ªçc v√† chunk")
            try:
                text_content = content.decode('utf-8')
                logger.info(f"   ƒê·ªçc ƒë∆∞·ª£c {len(text_content)} k√Ω t·ª±")
                processed = preprocess_text(text_content)
                file_chunks = chunk_text(
                    processed,
                    chunk_size=int(config.get("chunk_size", 800)),
                    chunk_overlap=int(config.get("chunk_overlap", 120)),
                    source=f"uploaded:{file.filename}"
                )
                logger.info(f"   Chunk th√†nh {len(file_chunks)} chunks, ch·ªçn {min(2, len(file_chunks))} chunks ƒë·∫ßu ti√™n")
                contexts_for_llm.extend([{"text": c["text"], "source": c["source"], "score": 1.0} for c in file_chunks[:2]])
                for i, ctx in enumerate(contexts_for_llm, 1):
                    logger.info(f"   File Context {i}: {ctx.get('text', '')[:100]}...")
            except Exception as e:
                logger.error(f"   ‚ùå L·ªói ƒë·ªçc file: {str(e)}")
                return JSONResponse(
                    status_code=400,
                    content={"error": f"Kh√¥ng th·ªÉ ƒë·ªçc file: {str(e)}"}
                )
    else:
        logger.info("   Kh√¥ng c√≥ file ƒë∆∞·ª£c upload")
    
    # RAG Search
    logger.info("üîç B·∫ÆT ƒê·∫¶U RAG SEARCH...")
    results = rag.search(question, top_k=top_k)
    logger.info(f"üìä T√¨m th·∫•y {len(results)} k·∫øt qu·∫£ t·ª´ RAG search")
    
    # Log t·ª´ng k·∫øt qu·∫£
    for i, r in enumerate(results, 1):
        score = r.get("score", 0.0)
        source = r.get("source", "unknown")
        text_preview = r.get("text", "")[:100] + "..." if len(r.get("text", "")) > 100 else r.get("text", "")
        logger.info(f"  [{i}] Score: {score:.4f} | Source: {source}")
        logger.info(f"      Preview: {text_preview}")
    
    # Filter theo similarity threshold
    similarity_threshold = float(config.get("similarity_threshold", 0.6))
    logger.info(f"üéØ L·ªçc theo similarity threshold: {similarity_threshold}")
    filtered = [r for r in results if float(r.get("score", 0.0)) >= similarity_threshold]
    logger.info(f"‚úÖ Sau khi l·ªçc: {len(filtered)} k·∫øt qu·∫£ ƒë·∫°t ng∆∞·ª°ng")
    
    # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng contexts
    contexts_max = int(config.get("contexts_max", 3))
    contexts_for_llm.extend(filtered[:contexts_max])
    logger.info(f"üì¶ T·ªïng c·ªông {len(contexts_for_llm)} contexts ƒë·ªÉ g·ª≠i ƒë·∫øn LLM (RAG: {len(filtered[:contexts_max])}, File: {len([c for c in contexts_for_llm if c.get('source', '').startswith('uploaded:')])})")
    
    # Log contexts ƒë∆∞·ª£c ch·ªçn
    for i, ctx in enumerate(contexts_for_llm, 1):
        logger.info(f"  Context {i}: Score={ctx.get('score', 0.0):.4f}, Source={ctx.get('source', 'unknown')}")
        logger.info(f"    Text: {ctx.get('text', '')[:200]}...")
    
    # Build prompt v√† g·ªçi LLM
    model_type = os.getenv("MODEL_TYPE", "openai").lower()
    logger.info(f"ü§ñ G·ªçi LLM: {model_type.upper()}")
    
    if model_type == "ollama":
        prompt = llm_client.build_prompt(question, contexts_for_llm)
        logger.info("üìÑ PROMPT ƒê∆Ø·ª¢C X√ÇY D·ª∞NG:")
        logger.info("-" * 80)
        logger.info(prompt[:1000] + "..." if len(prompt) > 1000 else prompt)
        logger.info("-" * 80)
        
        answer, meta = llm_client.answer(
            question, 
            contexts_for_llm
        )
    else:
        prompt = llm_client.build_prompt(question, contexts_for_llm)
        logger.info("üìÑ PROMPT ƒê∆Ø·ª¢C X√ÇY D·ª∞NG:")
        logger.info("-" * 80)
        logger.info(prompt[:1000] + "..." if len(prompt) > 1000 else prompt)
        logger.info("-" * 80)
        
        if image_urls:
            logger.info(f"üñºÔ∏è  C√≥ {len(image_urls)} ·∫£nh ƒë∆∞·ª£c g·ª≠i k√®m")
        if use_websearch:
            logger.info("üåê Web search ƒë∆∞·ª£c b·∫≠t")
        
        answer, meta = llm_client.answer(
            question, 
            contexts_for_llm, 
            image_urls=image_urls if image_urls else None,
            file_urls=None,
            use_websearch=use_websearch
        )
    
    elapsed_ms = int((time.perf_counter() - start) * 1000)
    logger.info(f"üí¨ C√¢u tr·∫£ l·ªùi nh·∫≠n ƒë∆∞·ª£c (ƒë·ªô d√†i: {len(answer)} k√Ω t·ª±)")
    logger.info(f"‚è±Ô∏è  T·ªïng th·ªùi gian x·ª≠ l√Ω: {elapsed_ms}ms")
    logger.info("=" * 80)
    
    return {
        "question": question,
        "answer": answer,
        "contexts": contexts_for_llm,
        "meta": meta,
        "latency_ms": elapsed_ms
    }


class StoryRequest(BaseModel):
    topic: Optional[str] = None  # Ch·ªß ƒë·ªÅ c√¢u chuy·ªán (v√≠ d·ª•: "giai c·∫•p c√¥ng nh√¢n", "li√™n minh giai c·∫•p")
    character: Optional[str] = "H·ªì Ch√≠ Minh"  # Nh√¢n v·∫≠t ch√≠nh (m·∫∑c ƒë·ªãnh l√† B√°c H·ªì)
    length: Optional[str] = "medium"  # "short" (3-5 c√¢u), "medium" (5-8 c√¢u), "long" (8-12 c√¢u)


@app.post("/story")
def generate_story(req: StoryRequest):
    """T·ª± ƒë·ªông t·∫°o m·ªôt c√¢u chuy·ªán v·ªÅ l·ªãch s·ª≠ c√°ch m·∫°ng."""
    start = time.perf_counter()
    
    logger.info("=" * 80)
    logger.info(f"üìñ Y√äU C·∫¶U T·∫†O C√ÇU CHUY·ªÜN")
    logger.info(f"üîß Tham s·ªë: topic={req.topic}, character={req.character}, length={req.length}")
    
    # X√¢y d·ª±ng prompt cho vi·ªác k·ªÉ chuy·ªán
    topic_text = f"v·ªÅ ch·ªß ƒë·ªÅ '{req.topic}'" if req.topic else "v·ªÅ ch·ªß nghƒ©a M√°c-L√™nin, kinh t·∫ø ch√≠nh tr·ªã ho·∫∑c l·ªãch s·ª≠ c√°ch m·∫°ng"
    
    length_instruction = {
        "short": "3-5 c√¢u",
        "medium": "5-8 c√¢u", 
        "long": "8-12 c√¢u"
    }.get(req.length, "5-8 c√¢u")
    
    story_prompt = (
        f"B·∫°n l√† tr·ª£ l√Ω t√™n ViVi. H√£y k·ªÉ m·ªôt c√¢u chuy·ªán {length_instruction} "
        f"v·ªÅ {req.character} {topic_text}. "
        "C√¢u chuy·ªán ph·∫£i s·ªëng ƒë·ªông, c√≥ c·∫£m x√∫c, mang t√≠nh gi√°o d·ª•c v√† truy·ªÅn c·∫£m h·ª©ng. "
        "H√£y k·ªÉ nh∆∞ m·ªôt ng∆∞·ªùi k·ªÉ chuy·ªán ch√¢n th·ª±c, kh√¥ng c·∫ßn m·ªü ƒë·∫ßu hay k·∫øt th√∫c trang tr·ªçng, "
        "ch·ªâ c·∫ßn k·ªÉ c√¢u chuy·ªán m·ªôt c√°ch t·ª± nhi√™n v√† h·∫•p d·∫´n."
    )
    
    # G·ªçi LLM ƒë·ªÉ t·∫°o c√¢u chuy·ªán
    model_type = os.getenv("MODEL_TYPE", "openai").lower()
    
    if model_type == "ollama":
        answer, meta = llm_client.answer(story_prompt, [])
    else:
        answer, meta = llm_client.answer(story_prompt, [])
    
    elapsed_ms = int((time.perf_counter() - start) * 1000)
    logger.info(f"üìñ C√¢u chuy·ªán ƒë√£ ƒë∆∞·ª£c t·∫°o (ƒë·ªô d√†i: {len(answer)} k√Ω t·ª±)")
    logger.info(f"‚è±Ô∏è  Th·ªùi gian x·ª≠ l√Ω: {elapsed_ms}ms")
    logger.info("=" * 80)
    
    return {
        "story": answer,
        "topic": req.topic,
        "character": req.character,
        "length": req.length,
        "meta": meta,
        "latency_ms": elapsed_ms
    }


class TTSRequest(BaseModel):
    text: str
    voice: Optional[str] = "nova"  # "alloy", "echo", "fable", "onyx", "nova", "shimmer"
    model: Optional[str] = "tts-1"  # "tts-1" ho·∫∑c "tts-1-hd"


@app.post("/tts")
def text_to_speech(req: TTSRequest):
    """
    Chuy·ªÉn ƒë·ªïi text th√†nh speech s·ª≠ d·ª•ng OpenAI TTS API.
    Tr·∫£ v·ªÅ file audio MP3.
    """
    try:
        model_type = os.getenv("MODEL_TYPE", "openai").lower()
        
        if model_type != "openai":
            return JSONResponse(
                status_code=400,
                content={"error": "TTS ch·ªâ h·ªó tr·ª£ khi s·ª≠ d·ª•ng OpenAI model"}
            )
        
        if not isinstance(llm_client, OpenAIClient):
            return JSONResponse(
                status_code=400,
                content={"error": "LLM client kh√¥ng ph·∫£i OpenAI client"}
            )
        
        # Gi·ªõi h·∫°n ƒë·ªô d√†i text ƒë·ªÉ tr√°nh t·ªën ph√≠
        max_length = 5000
        if len(req.text) > max_length:
            req.text = req.text[:max_length] + "..."
        
        audio_data = llm_client.text_to_speech(
            text=req.text,
            voice=req.voice,
            model=req.model
        )
        
        return Response(
            content=audio_data,
            media_type="audio/mpeg",
            headers={
                "Content-Disposition": "inline; filename=speech.mp3"
            }
        )
    except Exception as e:
        logger.error(f"L·ªói TTS: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={"error": f"L·ªói t·∫°o TTS: {str(e)}"}
        )


def _set_runtime_env_for_mac():
    """Thi·∫øt l·∫≠p bi·∫øn m√¥i tr∆∞·ªùng ƒë·ªÉ server ·ªïn ƒë·ªãnh."""
    os.environ.setdefault("PYTORCH_MPS_DISABLE", "1")
    os.environ.setdefault("TORCH_MPS_ENABLED", "0")
    os.environ.setdefault("OMP_NUM_THREADS", "1")
    os.environ.setdefault("MKL_THREADING_LAYER", "SEQUENTIAL")


def _parse_args():
    parser = argparse.ArgumentParser(description="Ch·∫°y MLN131 FastAPI server")
    parser.add_argument("--host", default="127.0.0.1", help="Host (m·∫∑c ƒë·ªãnh 127.0.0.1)")
    parser.add_argument("--port", type=int, default=2000, help="Port (m·∫∑c ƒë·ªãnh 2000)")
    parser.add_argument("--reload", action="store_true", help="B·∫≠t reload khi ph√°t tri·ªÉn")
    return parser.parse_args()


if __name__ == "__main__":
    _set_runtime_env_for_mac()
    args = _parse_args()
    if args.reload:
        uvicorn.run("src.main:app", host=args.host, port=args.port, reload=True, log_level="info")
    else:
        uvicorn.run(app, host=args.host, port=args.port, reload=False, log_level="info")
